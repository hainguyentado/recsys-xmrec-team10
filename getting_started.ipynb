{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "separated-blake",
   "metadata": {},
   "source": [
    "## Note\n",
    "In this notebook we will load a trained GMF++ model, and go over the evaluation procedure. The GMF++ is based on simple model introduced by [He et al](https://arxiv.org/abs/1708.05031). You can try to adapt other models such as MLP and NMF. The [original implementation](https://github.com/hexiangnan/neural_collaborative_filtering/tree/4aab159e81c44b062c091bdaed0ab54ac632371f) as well as other implemntations are available for single market settings.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "genuine-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import resource\n",
    "import sys\n",
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "\n",
    "sys.path.insert(1, 'src')\n",
    "from model import Model\n",
    "from utils import *\n",
    "from data import *\n",
    "from train_baseline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coordinate-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = create_arg_parser()\n",
    "tgt_market = 't1'\n",
    "src_markets = 's1'\n",
    "\n",
    "args = parser.parse_args(f'--tgt_market {tgt_market} --src_markets {src_markets}'.split()) #\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "    torch.cuda.set_device(0)\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() and args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "instant-drunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is GMF++!\n",
      "GMF(\n",
      "  (embedding_user): Embedding(9164, 8)\n",
      "  (embedding_item): Embedding(10341, 8)\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Pretrained weights from checkpoints/t1_s1_toytest.model are loaded!\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model_dir = f'checkpoints/{tgt_market}_{src_markets}_toytest.model'\n",
    "id_bank_dir = f'checkpoints/{tgt_market}_{src_markets}_toytest.pickle'\n",
    "\n",
    "valid_run = f'valid_{tgt_market}_{src_markets}_toytest.tsv'\n",
    "\n",
    "with open(id_bank_dir, 'rb') as centralid_file:\n",
    "    my_id_bank = pickle.load(centralid_file)\n",
    "\n",
    "mymodel = Model(args, my_id_bank)\n",
    "mymodel.load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "southern-salad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading DATA/t1/train.tsv\n",
      "loaded target data!\n",
      "loaded target test and validation data!\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## Target Market data\n",
    "############\n",
    "tgt_train_data_dir = os.path.join(args.data_dir, args.tgt_market, 'train.tsv')\n",
    "tgt_train_ratings = pd.read_csv(tgt_train_data_dir, sep='\\t')\n",
    "\n",
    "print(f'loading {tgt_train_data_dir}')\n",
    "tgt_task_generator = TaskGenerator(tgt_train_ratings, my_id_bank)\n",
    "print('loaded target data!')\n",
    "\n",
    "tgt_valid_dataloader = tgt_task_generator.instance_a_market_valid_dataloader(args.tgt_market_valid, args.batch_size)\n",
    "tgt_test_dataloader = tgt_task_generator.instance_a_market_valid_dataloader(args.tgt_market_test, args.batch_size)\n",
    "print('loaded target test and validation data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-release",
   "metadata": {},
   "source": [
    "Here, we run the prediction step on both validation and test sets. \n",
    "\n",
    "Then, we write the output files in the format required for the submission and create the Zip file for submission.\n",
    "\n",
    "Finally, we run the `validate_subsmission.py` to make sure that the structure of the Zip file is okay. In addition, we evaluate the model on the `valid` set.\n",
    "\n",
    "**Note**: You need to run the script twice for both target markets (i.e., `t1` and `t2`). So, the code writes the prediction files in both `sample_run/t1/` and `sample_run/t2/` directories. Otherwise, your submission file will not pass the file structure test of `validate_submission.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = './baseline_outputs/sample_run/'\n",
    "\n",
    "def write_run_file(run_mf, file_address):\n",
    "    with open(file_address, 'w') as fo:\n",
    "        fo.write('userId\\titemId\\tscore\\n')\n",
    "        for u_id in run_mf:\n",
    "            for p_id in run_mf[u_id]:\n",
    "                fo.write('{}\\t{}\\t{}\\n'.format(u_id, p_id, run_mf[u_id][p_id]))\n",
    "\n",
    "valid_run_mf = mymodel.predict(tgt_valid_dataloader)\n",
    "test_run_mf = mymodel.predict(tgt_test_dataloader)\n",
    "\n",
    "write_run_file(valid_run_mf, path.join(run_dir, tgt_market, 'valid_pred.tsv'))\n",
    "write_run_file(test_run_mf, path.join(run_dir, tgt_market, 'test_pred.tsv'))\n",
    "\n",
    "# get full evaluation on validation set using pytrec_eval.\n",
    "tgt_valid_qrel = read_qrel_file('DATA/t1/valid_qrel.tsv')\n",
    "task_ov, task_ind = get_evaluations_final(valid_run_mf, tgt_valid_qrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the run files into a single archive to prepare for submission    \n",
    "! cd {run_dir} && zip -r ../sample_run.zip ./\n",
    "\n",
    "# Run the validate_submission.py script to check if the file format is okay and get the performance on validation set.\n",
    "! python validate_submission.py ./baseline_outputs/sample_run.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
